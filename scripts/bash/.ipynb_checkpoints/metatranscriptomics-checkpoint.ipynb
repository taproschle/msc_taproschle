{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metatranscriptomics Workflow\n",
    "\n",
    "#### By TomÃ¡s Alonso Proschle Donoso (MSc. PUC Chile)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Environments installation using miniconda3\n",
    "\n",
    "Miniconda3 is a lightweight distribution of the Conda package manager designed for Python and scientific computing. It allows you to create isolated environments with different versions of Python and packages. This is useful for managing dependencies and creating reproducible workflows. Miniconda3 simplifies dependency management, provides isolated environments for different programs, enables portability, and ensures version control. The version used was 23.3.1 and can be found on this [link](https://docs.conda.io/en/latest/miniconda.html).\n",
    "\n",
    "For the preprocessing of the libraries, the following conda environment will be generated by running the following code on the console:\n",
    "\n",
    "`\n",
    "conda create --name preprocessing -c bioconda vsearch trimmomatic sortmerna bbmap fastqc multiqc\n",
    "`\n",
    "\n",
    "For the functional annotation procedure the following conda environment will be used:\n",
    "\n",
    "`\n",
    "conda create --name fun-annotation -c bioconda hisat2 samtools htseq\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p 1_reads_raw\n",
    "\n",
    "# The readings are imported to the 1_reads_raw folder, for this analysis the readings were imported locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list for the loop\n",
    "ls 1_reads_raw/*.gz | sed 's/_..fastq.gz//; s/1_reads_raw.//' | sort -u > list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pre-processing of readings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Analysis of readings using FastQC and MultiQC\n",
    "\n",
    "Before starting to work with the readings, it is important to verify the quality of the samples. For this purpose, the FastQC program is used, which generates a complete report of important parameters such as the quality distribution of the reads. This report is individual for each file, so viewing one by one for a large number of files can be tedious, for this it is possible to use the MultiQC program which groups all the reports and delivers a summary of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate the conda environment\n",
    "conda activate preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd 1_reads_raw\n",
    "mkdir -p fastqc_out\n",
    "\n",
    "fastqc *.gz -out fastqc_out/\n",
    "multiqc fastqc_out/ --outdir fastqc_out/\n",
    "\n",
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting file `multiqc_report.html` contains the display of the qualities of the readings, in this report it is possible to see the presence of adapters, average quality of the readings, presence of missmatches, among others. For RNA-Seq data it is very logical that there is a large number of duplicate reads, so the warning given can be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Trimming of poor quality readings using Trimmomatic\n",
    "\n",
    "Looking at the previous report, it must be decided whether to cut or not, what minimum score to use and if it is necessary to remove adapters. In this case the Trimmomatic program is not instructed to remove Illumina adapters as it does not have them, this was possible to verify with the MultiQC report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p 2_reads_trim\n",
    "\n",
    "# Trimmomatic call\n",
    "while read i; do\n",
    "trimmomatic PE -threads 16 -trimlog 2_reads_trim/\"$i\".trimlog 1_reads_raw/\"$i\"_1.fastq.gz 1_reads_raw/\"$i\"_2.fastq.gz 2_reads_trim/\"$i\"_1_trim.fastq.gz 2_reads_trim/\"$i\"_1_trim_unpaired.fastq.gz 2_reads_trim/\"$i\"_2_trim.fastq.gz 2_reads_trim/\"$i\"_2_trim_unpaired.fastq.gz LEADING:3 TRAILING:3 SLIDINGWINDOW:3:15 MINLEN:36\n",
    "done < list\n",
    "\n",
    "# The evaluation of the readings is performed again\n",
    "mkdir -p 2_reads_trim/fastqc_out\n",
    "cd 2_reads_trim\n",
    "ls *trim.fastq.gz | xargs fastqc --outdir fastqc_out/\n",
    "\n",
    "# MultiQC report\n",
    "multiqc fastqc_out/ --outdir fastqc_out/\n",
    "\n",
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Filter readings using VSearch\n",
    "\n",
    "After trimming the readings, a filter is performed taking into account the average expected error of the reading predicted by the phred score. For more information visit the following [page](https://www.drive5.com/usearch/manual/exp_errs.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p 3_reads_preproc\n",
    "\n",
    "while read i; do\n",
    "vsearch --fastq_filter 2_reads_trim/\"$i\"_1_trim.fastq.gz --reverse 2_reads_trim/\"$i\"_2_trim.fastq.gz --gzip_decompress --threads 0 --fastq_maxee 2.0 --fastqout 3_reads_preproc/\"$i\"_1.fastq --fastqout_rev 3_reads_preproc/\"$i\"_2.fastq\n",
    "done < list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Separation of reads between rRNA and mRNA using SortMeRNA\n",
    "\n",
    "Once the readings have been filtered for quality, it is necessary to separate those readings that correspond to rRNA, since the subsequent analysis is based on gene expression, which is closely linked to mRNA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# database download\n",
    "mkdir -p 0_databases/sortmerna/\n",
    "\n",
    "cd 0_databases/sortmerna/\n",
    "wget https://github.com/biocore/sortmerna/releases/download/v4.3.4/database.tar.gz\n",
    "tar -xzvf database.tar.gz\n",
    "\n",
    "cd ../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd 3_reads_preproc\n",
    "\n",
    "while read i; do\n",
    "mkdir -p \"$i\"\n",
    "sortmerna --ref ../0_databases/sortmerna/smr_v4.3_sensitive_db.fasta --reads \"$i\"_1.fastq --reads \"$i\"_2.fastq --workdir \"$i\" --fastx --out2 --threads 16\n",
    "done < ../list\n",
    "\n",
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files generated by the program contain reads that align with rRNA databases, so it is necessary to exclude them. For this purpose, the BBMap `filterbyname` script is used, which from the tags of the reads is able to create a new file with the information coming from mRNAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd 3_reads_preproc\n",
    "\n",
    "while read i; do\n",
    "filterbyname.sh in=\"$i\"_1.fastq out=\"$i\"_1_mRNA.fastq names=\"$i\"/out/aligned_fwd.fq include=f\n",
    "filterbyname.sh in=\"$i\"_2.fastq out=\"$i\"_2_mRNA.fastq names=\"$i\"/out/aligned_rev.fq include=f\n",
    "done < ../list\n",
    "\n",
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it is possible that some of the readings may be missing due to some of the filters performed in the previous steps (Trimmomatic and VSearch), they are repaired using the `repair` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p 4_reads_prepared\n",
    "\n",
    "while read i; do\n",
    "repair.sh in=3_reads_preproc/\"$i\"_1_mRNA.fastq in2=3_reads_preproc/\"$i\"_2_mRNA.fastq out=4_reads_prepared/\"$i\"_1.fastq out2=4_reads_prepared/\"$i\"_2.fastq\n",
    "done < list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Final quality check\n",
    "\n",
    "For the last step of this section, the final quality of the libraries is verified before proceeding to the functional analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd 4_reads_prepared\n",
    "mkdir -p fastqc_out\n",
    "\n",
    "fastqc *.fastq -out fastqc_out/\n",
    "multiqc fastqc_out/\n",
    "\n",
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Functional analysis\n",
    "\n",
    "For the genes assignment the `fun-annotation` environment will be used. The genomes of reference used are the following, which will be copied into a new genomes folder locally:\n",
    "\n",
    "- [*Bifidobacterium bifidum* JCM 1254](https://www.bv-brc.org/view/Genome/398514.7)\n",
    "- [*Bifidobacterium breve* DSM 20213 = JCM 1192](https://www.bv-brc.org/view/Genome/518634.20)\n",
    "- [*Bifidobacterium longum* subsp. infantis ATCC 15697 = JCM 1222](https://www.bv-brc.org/view/Genome/391904.5)\n",
    "- [*Bacteroides thetaiotaomicron* VPI-5482](https://www.bv-brc.org/view/Genome/226186.12)\n",
    "- [*Lachnoclostridium symbiosum* WAL-14673](https://www.bv-brc.org/view/Genome/742741.3)\n",
    "- [*Escherichia coli* str. K-12 substr. MG1655](https://www.bv-brc.org/view/Genome/511145.12)\n",
    "- [*Pediococcus acidilactici* strain PMC65](https://www.bv-brc.org/view/Genome/1254.353)\n",
    "\n",
    "The files downloaded for each genome were:\n",
    "\n",
    "- Protein Sequences in FASTA (`.faa`)\n",
    "- Genomic Sequences in FASTA (`.fna`)\n",
    "- Genomic features in Generic Feature Format format (`.gff`)\n",
    "- Pathway assignments in tab-delimited format (`.pathway.tab`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment activation\n",
    "conda activate fun-annotation\n",
    "\n",
    "mkdir -p 5_functional_analysis\n",
    "\n",
    "mkdir -p 0_genomes/reference/\n",
    "\n",
    "# copy manually genomes folder containing fna, faa, gff, pathways files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Reference generation\n",
    "\n",
    "For each experiment, the reference genomes and annotation were created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat list | sed 's/_r.//g' | uniq | while read exp; do\n",
    "    # genome reference\n",
    "    cat list | sed 's/_r.//g' | uniq | sed '1d' | grep -v \"${exp}\" | while read expp; do\n",
    "        cat 0_genomes/${expp}/${expp}.fna\n",
    "    done > 0_genomes/reference/${exp}.fasta\n",
    "    # contigs ids\n",
    "    cat list | sed 's/_r.//g' | uniq | sed '1d' | grep -v \"${exp}\" | while read expp; do\n",
    "        grep \">\" 0_genomes/${expp}/${expp}.fna | cut -d' ' -f1 | sed 's/>//g'\n",
    "    done > 0_genomes/reference/${exp}_contig_ids.txt\n",
    "    # gff reference\n",
    "    cat list | sed 's/_r.//g' | uniq | sed '1d' | grep -v \"${exp}\" | while read expp; do\n",
    "        cat 0_genomes/${expp}/${expp}.gff | sed 's/accn|//g'\n",
    "    done > 0_genomes/reference/${exp}.gff\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Alignment and counts file generation\n",
    "\n",
    "The following cell aligns the genetic reads to a reference genome using HISAT2, sorts and indexes the aligned reads using SAMtools, and then counts the aligned reads using HTSeq. The output includes alignment files, sorted BAM files, and count files that provide information about the aligned reads in relation to the genome's features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p 5_functional_analysis/aln/\n",
    "mkdir -p 5_functional_analysis/counts/\n",
    "\n",
    "# hisat2 indexing genomes\n",
    "cat list | sed 's/_r.//g' | uniq | while read exp; do\n",
    "    hisat2-build 0_genomes/reference/${exp}.fasta 0_genomes/reference/${exp}\n",
    "done\n",
    "\n",
    "cat list | while read exp; do\n",
    "    expp=$(echo \"$exp\" | sed 's/_r.//g')\n",
    "    # align reads\n",
    "    hisat2 -x 0_genomes/reference/${expp} -1 4_reads_prepared/${exp}_1.fastq -2 4_reads_prepared/${exp}_2.fastq --threads 16 -S 5_functional_analysis/aln/${exp}.sam\n",
    "    # sort aligned reads\n",
    "    samtools view -S -b 5_functional_analysis/aln/${exp}.sam -@ 16 > 5_functional_analysis/aln/${exp}.bam \n",
    "    samtools sort 5_functional_analysis/aln/${exp}.bam -@ 16 -o 5_functional_analysis/aln/${exp}_srt.bam \n",
    "    samtools index 5_functional_analysis/aln/${exp}_srt.bam -@ 16\n",
    "    # count aligned reads\n",
    "    htseq-count --order=pos --stranded=yes --type=CDS --idattr=ID --additional-attr=locus_tag --additional-attr=product --additional-attr=gene --counts_output=5_functional_analysis/counts/${exp}_counts.tsv --nprocesses=16 5_functional_analysis/aln/${exp}_srt.bam 0_genomes/reference/${expp}.gff\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 File formatting\n",
    "\n",
    "The following scripts prepare the results obtained from the previous cells and format them into the required structure for further processing with R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merges tables of counts\n",
    "cat << EOF > merge_tables.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "file_df1 = sys.argv[1]\n",
    "file_df2 = sys.argv[2]\n",
    "file_df3 = sys.argv[3]\n",
    "out_file = sys.argv[4]\n",
    "\n",
    "df1 = pd.read_csv(file_df1, sep=\"\\t\", names=[\"ID\", \"locus_tag\", \"product\", \"gene\", \"counts_r1\"])\n",
    "df2 = pd.read_csv(file_df2, sep=\"\\t\", names=[\"ID\", \"locus_tag\", \"product\", \"gene\", \"counts_r2\"])\n",
    "df3 = pd.read_csv(file_df3, sep=\"\\t\", names=[\"ID\", \"locus_tag\", \"product\", \"gene\", \"counts_r3\"])\n",
    "\n",
    "merged_df = df1.merge(df2, on=[\"ID\", \"locus_tag\", \"product\", \"gene\"], how=\"outer\").merge(df3, on=[\"ID\", \"locus_tag\", \"product\", \"gene\"], how=\"outer\")\n",
    "\n",
    "merged_df.to_csv(out_file, sep=\"\\t\", index=False)\n",
    "EOF\n",
    "\n",
    "chmod +x merge_tables.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs the script\n",
    "mkdir -p 5_functional_analysis/results\n",
    "cd 5_functional_analysis/\n",
    "\n",
    "cat ../list | sed 's/_r.//g' | uniq | while read exp; do    \n",
    "    ../merge_tables.py counts/${exp}_r1_counts.tsv counts/${exp}_r2_counts.tsv counts/${exp}_r3_counts.tsv results/${exp}_counts.tsv\n",
    "    sed -i \"s/counts_r/${exp}_R/g\" results/${exp}_counts.tsv\n",
    "done\n",
    "\n",
    "cd ..\n",
    "rm merge_tables.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merges to a final table\n",
    "cd 5_functional_analysis/\n",
    "\n",
    "cat << EOF > merge_tables.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "all = pd.read_csv(\"results/all_counts.tsv\", sep=\"\\t\")\n",
    "bbif = pd.read_csv(\"results/bbif_counts.tsv\", sep=\"\\t\")\n",
    "bbre = pd.read_csv(\"results/bbre_counts.tsv\", sep=\"\\t\")\n",
    "binf = pd.read_csv(\"results/binf_counts.tsv\", sep=\"\\t\")\n",
    "bthe = pd.read_csv(\"results/bthe_counts.tsv\", sep=\"\\t\")\n",
    "lsym = pd.read_csv(\"results/lsym_counts.tsv\", sep=\"\\t\")\n",
    "ecol = pd.read_csv(\"results/ecol_counts.tsv\", sep=\"\\t\")\n",
    "paci = pd.read_csv(\"results/paci_counts.tsv\", sep=\"\\t\")\n",
    "\n",
    "df_merged = all.merge(bbif, on=[\"ID\", \"locus_tag\", \"product\", \"gene\"], how=\"outer\")\n",
    "df_merged = df_merged.merge(bbre, on=[\"ID\", \"locus_tag\", \"product\", \"gene\"], how=\"outer\")\n",
    "df_merged = df_merged.merge(binf, on=[\"ID\", \"locus_tag\", \"product\", \"gene\"], how=\"outer\")\n",
    "df_merged = df_merged.merge(bthe, on=[\"ID\", \"locus_tag\", \"product\", \"gene\"], how=\"outer\")\n",
    "df_merged = df_merged.merge(lsym, on=[\"ID\", \"locus_tag\", \"product\", \"gene\"], how=\"outer\")\n",
    "df_merged = df_merged.merge(ecol, on=[\"ID\", \"locus_tag\", \"product\", \"gene\"], how=\"outer\")\n",
    "df_merged = df_merged.merge(paci, on=[\"ID\", \"locus_tag\", \"product\", \"gene\"], how=\"outer\")\n",
    "\n",
    "df_merged.to_csv(\"counts.tsv\", sep=\"\\t\", index=False)\n",
    "EOF\n",
    "\n",
    "chmod +x merge_tables.py\n",
    "./merge_tables.py\n",
    "rm merge_tables.py\n",
    "\n",
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final result is a file called `counts.tsv` inside the `5_functional_analysis` folder, which will be further analyzed with R using DESeq2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also of interest to know the number of readings assigned to each microorganism, the following scripts perform this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat << EOF > genome_map_count.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import pysam\n",
    "\n",
    "def count_paired_reads(bam_file, genome_ids_file):\n",
    "    genome_ids = read_genome_ids(genome_ids_file)\n",
    "    counts = {genome_id: 0 for genome_id in genome_ids}\n",
    "    samfile = pysam.AlignmentFile(bam_file, \"rb\")\n",
    "\n",
    "    for genome_id in genome_ids:\n",
    "        for read in samfile.fetch(genome_id):\n",
    "            if read.is_paired and not read.is_secondary and not read.is_supplementary:\n",
    "                counts[genome_id] += 1\n",
    "\n",
    "    samfile.close()\n",
    "    return counts\n",
    "\n",
    "def read_genome_ids(genome_ids_file):\n",
    "    with open(genome_ids_file, \"r\") as file:\n",
    "        genome_ids = [line.strip() for line in file]\n",
    "\n",
    "    return genome_ids\n",
    "\n",
    "bam_file_path = sys.argv[1]\n",
    "genome_ids_file_path = sys.argv[2]\n",
    "\n",
    "paired_reads_counts = count_paired_reads(bam_file_path, genome_ids_file_path)\n",
    "\n",
    "for genome_id, count in paired_reads_counts.items():\n",
    "    print(f\"{genome_id}\\t{count}\")\n",
    "EOF\n",
    "\n",
    "chmod +x genome_map_count.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p 5_functional_analysis/library_composition\n",
    "\n",
    "# codes for each contig specified in the genome obtained\n",
    "cat << EOF > 5_functional_analysis/library_composition/exp_code_genomes.csv\n",
    "BQJY010000..,bbif\n",
    "AP012324,bbre\n",
    "NC_011593,binf\n",
    "NC_004..3,bthe\n",
    "NZ_GL834...,lsym\n",
    "NC_000913,ecol\n",
    "CP053421,paci\n",
    "EOF\n",
    "\n",
    "cat list | while read exp; do\n",
    "    expp=$(echo \"$exp\" | sed 's/_r.//g')\n",
    "    ./genome_map_count.py 5_functional_analysis/aln/${exp}_srt.bam 0_genomes/reference/${expp}_contig_ids.txt > 5_functional_analysis/library_composition/${exp}_comp.temp\n",
    "    cat 5_functional_analysis/library_composition/exp_code_genomes.csv | while read line; do\n",
    "        pat=$(echo \"$line\" | cut -d',' -f1)\n",
    "        spe=$(echo \"$line\" | cut -d',' -f2)\n",
    "        count=$(grep \"$pat\" 5_functional_analysis/library_composition/${exp}_comp.temp | awk '{sum += $2} END {print sum}')\n",
    "        echo -e \"${spe}\\t${count}\"\n",
    "    done > 5_functional_analysis/library_composition/${exp}_comp.txt\n",
    "    rm 5_functional_analysis/library_composition/${exp}_comp.temp\n",
    "done\n",
    "\n",
    "rm genome_map_count.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat << EOF > tidy_data.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "input_directory = sys.argv[1]\n",
    "output_file = sys.argv[2]\n",
    "\n",
    "# Initialize a dictionary to store the data\n",
    "data = {}\n",
    "\n",
    "# Process each file in the input directory\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        filepath = os.path.join(input_directory, filename)\n",
    "        with open(filepath, \"r\") as file:\n",
    "            experiment_name = filename.split(\"_\")[0]\n",
    "            replica_number = filename.split(\"_\")[1].split(\".\")[0]\n",
    "\n",
    "            for line in file:\n",
    "                line = line.strip().split(\"\\t\")\n",
    "                species = line[0]\n",
    "                num = line[1] if len(line) == 2 else \"0\"\n",
    "\n",
    "                if experiment_name not in data:\n",
    "                    data[experiment_name] = []\n",
    "\n",
    "                data[experiment_name].append([experiment_name, replica_number, species, num])\n",
    "\n",
    "# Write the data to the output file\n",
    "with open(output_file, \"w\") as file:\n",
    "    file.write(\"exp\\treplicate\\tspecies\\tnum\\n\")\n",
    "    for experiment_name, rows in data.items():\n",
    "        for row in rows:\n",
    "            file.write(\"\\t\".join(row) + \"\\n\")\n",
    "\n",
    "EOF\n",
    "\n",
    "chmod +x tidy_data.py\n",
    "\n",
    "./tidy_data.py 5_functional_analysis/library_composition/ 5_functional_analysis/library_composition/comp_data.tsv\n",
    "\n",
    "rm tidy_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the previous scripts is the `comp_data.tsv` file, which is to be plotted with R."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
